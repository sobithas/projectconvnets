{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10899761,"sourceType":"datasetVersion","datasetId":6774065},{"sourceId":10909942,"sourceType":"datasetVersion","datasetId":6781730},{"sourceId":225164453,"sourceType":"kernelVersion"},{"sourceId":275799,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":236180,"modelId":257866}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install torch torchvision matplotlib opencv-python\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T06:49:46.069757Z","iopub.execute_input":"2025-03-14T06:49:46.070087Z","iopub.status.idle":"2025-03-14T06:49:49.771082Z","shell.execute_reply.started":"2025-03-14T06:49:46.070060Z","shell.execute_reply":"2025-03-14T06:49:49.769708Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.5)\nRequirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.3)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.2)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.9.0.post0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->torchvision) (2024.2.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import torch\nimport torchvision\nimport torch.nn as nn\nimport torchvision.models as models\nfrom torchvision.models.convnext import convnext_tiny\nfrom torchvision.models.detection import FasterRCNN\nfrom torchvision.models.detection.rpn import AnchorGenerator\n\n# Load device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load ConvNeXt-Tiny as the feature extractor\ndef get_convnext_tiny_backbone():\n    model = convnext_tiny(weights=None)  # No pretrained weights\n    checkpoint = torch.load(\"/kaggle/input/featuremodel/tensorflow2/default/1/best_convnext_tiny3.pth\", map_location=device)\n    \n    # Fix key mismatch issue\n    model_dict = model.state_dict()\n    checkpoint = {k: v for k, v in checkpoint.items() if k in model_dict}\n    model_dict.update(checkpoint)\n    model.load_state_dict(model_dict, strict=False)\n\n    # Remove the classification head\n    backbone = nn.Sequential(*list(model.children())[:-1])  # Remove last layer\n    backbone.out_channels = 768  # ConvNeXt-Tiny last feature map has 768 channels\n    return backbone\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T06:57:10.346088Z","iopub.execute_input":"2025-03-14T06:57:10.346492Z","iopub.status.idle":"2025-03-14T06:57:10.353154Z","shell.execute_reply.started":"2025-03-14T06:57:10.346464Z","shell.execute_reply":"2025-03-14T06:57:10.351635Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def get_faster_rcnn_model():\n    backbone = get_convnext_tiny_backbone()\n\n    # Define RPN anchor generator\n    anchor_generator = AnchorGenerator(\n        sizes=((32, 64, 128, 256, 512),),\n        aspect_ratios=((0.5, 1.0, 2.0),) * 5\n    )\n\n    # Define ROI Pooling\n    roi_pooler = torchvision.ops.MultiScaleRoIAlign(\n        featmap_names=[\"0\"], output_size=7, sampling_ratio=2\n    )\n\n    # Define Faster R-CNN model\n    model = FasterRCNN(\n        backbone,\n        num_classes=5,  # 4 disease classes + 1 background\n        rpn_anchor_generator=anchor_generator,\n        box_roi_pool=roi_pooler\n    )\n    \n    return model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T06:57:57.148058Z","iopub.execute_input":"2025-03-14T06:57:57.148440Z","iopub.status.idle":"2025-03-14T06:57:57.154414Z","shell.execute_reply.started":"2025-03-14T06:57:57.148408Z","shell.execute_reply":"2025-03-14T06:57:57.153214Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Initialize the model\nmodel = get_faster_rcnn_model()\nmodel.to(device)\nmodel.eval()  # Set to evaluation mode\n\nprint(\"Model loaded successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T06:58:21.541892Z","iopub.execute_input":"2025-03-14T06:58:21.542285Z","iopub.status.idle":"2025-03-14T06:58:22.454855Z","shell.execute_reply.started":"2025-03-14T06:58:21.542249Z","shell.execute_reply":"2025-03-14T06:58:22.453339Z"}},"outputs":[{"name":"stdout","text":"Model loaded successfully!\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import cv2\nimport numpy as np\nimport torchvision.transforms as transforms\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\n# Define class labels\nCLASS_NAMES = [\"Background\", \"Black Spot\", \"Black Gills\", \"White Spot\", \"AHPND\"]\n\n# Image preprocessing function\ndef preprocess_image(image_path):\n    image = Image.open(image_path).convert(\"RGB\")  # Convert to RGB\n    transform = transforms.Compose([\n        transforms.ToTensor(),  # Convert to tensor\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize\n    ])\n    image = transform(image)\n    return image\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T07:00:18.779940Z","iopub.execute_input":"2025-03-14T07:00:18.780419Z","iopub.status.idle":"2025-03-14T07:00:18.788057Z","shell.execute_reply.started":"2025-03-14T07:00:18.780383Z","shell.execute_reply":"2025-03-14T07:00:18.786533Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"\n# Load and preprocess an example image\nimage_path = \"/kaggle/input/preprocessedimgs/preprocessed/white_spots/White_spots  (1).jpg\"  # Change to your test image path\nimage = preprocess_image(image_path).to(device)\n\n# Add batch dimension\nimage = image.unsqueeze(0)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T07:02:56.486740Z","iopub.execute_input":"2025-03-14T07:02:56.487175Z","iopub.status.idle":"2025-03-14T07:02:56.513623Z","shell.execute_reply.started":"2025-03-14T07:02:56.487136Z","shell.execute_reply":"2025-03-14T07:02:56.512835Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# Run the model on the input image\nwith torch.no_grad():\n    predictions = model(image)\n\n# Extract predictions\nboxes = predictions[0]['boxes'].cpu().numpy()\nlabels = predictions[0]['labels'].cpu().numpy()\nscores = predictions[0]['scores'].cpu().numpy()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T07:02:59.684922Z","iopub.execute_input":"2025-03-14T07:02:59.685446Z","iopub.status.idle":"2025-03-14T07:03:01.044405Z","shell.execute_reply.started":"2025-03-14T07:02:59.685408Z","shell.execute_reply":"2025-03-14T07:03:01.042907Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"import torch\nimport torchvision\nimport torch.nn as nn\nimport torchvision.models as models\nfrom torchvision.models.convnext import convnext_tiny\nfrom torchvision.models.detection import FasterRCNN\nfrom torchvision.models.detection.rpn import AnchorGenerator\n\n# Load device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load ConvNeXt-Tiny as the feature extractor\ndef get_convnext_tiny_backbone():\n    convnext_tiny_model = convnext_tiny(pretrained=False)  # No pretrained weights\n    checkpoint = torch.load('/kaggle/input/featuremodel/tensorflow2/default/1/best_convnext_tiny3.pth', map_location=device)\n    \n    # Filter out unnecessary keys\n    model_dict = convnext_tiny_model.state_dict()\n    checkpoint_dict = {k: v for k, v in checkpoint.items() if k in model_dict}\n    model_dict.update(checkpoint_dict)\n    \n    convnext_tiny_model.load_state_dict(model_dict)\n\n    # Remove the classification head\n    backbone = nn.Sequential(*list(convnext_tiny_model.children())[:-1])  # Remove last layer\n    backbone.out_channels = 768  # ConvNeXt-Tiny last feature map has 768 channels\n    \n    return backbone\n\n# Define Faster R-CNN model\ndef get_faster_rcnn_model():\n    backbone = get_convnext_tiny_backbone()\n\n    # Define new anchor generator\n    anchor_generator = AnchorGenerator(\n        sizes=((32, 64, 128, 256, 512),),\n        aspect_ratios=((0.5, 1.0, 2.0),) * 5\n    )\n\n    # Define RoI Pooling\n    roi_pooler = torchvision.ops.MultiScaleRoIAlign(\n        featmap_names=[\"0\"], output_size=7, sampling_ratio=2\n    )\n\n    # Define Faster R-CNN Model\n    model = FasterRCNN(\n        backbone,\n        num_classes=5,  # 4 disease classes + 1 background\n        rpn_anchor_generator=anchor_generator,\n        box_roi_pool=roi_pooler\n    )\n\n    return model\n\n# Initialize the model\nmodel = get_faster_rcnn_model()\nmodel.to(device)\nmodel.eval()  # Set to evaluation mode\n\nprint(\"Model loaded successfully!\")\n\n# Image processing function\nimport cv2\nimport numpy as np\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\nCLASS_NAMES = ['Background', 'Black Gills', 'Black Spot', 'White Spot','Healthy']\n\ndef preprocess_image(image_path):\n    image = cv2.imread(image_path)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert to RGB\n\n    transform = transforms.Compose([\n        transforms.ToTensor(),  # Convert to tensor\n        transforms.Normalize(mean=[0.480, 0.448, 0.397], std=[0.229, 0.224, 0.225])  # Normalize\n    ])\n\n    image = transform(image)\n    return image\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T07:38:02.547456Z","iopub.execute_input":"2025-03-14T07:38:02.547815Z","iopub.status.idle":"2025-03-14T07:38:03.455006Z","shell.execute_reply.started":"2025-03-14T07:38:02.547787Z","shell.execute_reply":"2025-03-14T07:38:03.453741Z"}},"outputs":[{"name":"stdout","text":"Model loaded successfully!\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"import torch\nimport torchvision\nimport torch.nn as nn\nimport torchvision.models as models\nfrom torchvision.models.detection import FasterRCNN\nfrom torchvision.models.detection.rpn import AnchorGenerator\nimport cv2\nimport numpy as np\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom PIL import Image\n\n# Load device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Class labels\nCLASS_NAMES = ['Background', 'Black Gills', 'Black Spot', 'White Spot', 'Healthy']\n\n# Load ConvNeXt-Tiny as the feature extractor\ndef get_convnext_tiny_backbone():\n    convnext_tiny_model = models.convnext_tiny(pretrained=False)  # No pretrained weights\n    checkpoint = torch.load('/kaggle/input/featuremodel/tensorflow2/default/1/best_convnext_tiny3.pth', map_location=device)\n    \n    # Load state dict\n    model_dict = convnext_tiny_model.state_dict()\n    checkpoint_dict = {k: v for k, v in checkpoint.items() if k in model_dict}\n    model_dict.update(checkpoint_dict)\n    convnext_tiny_model.load_state_dict(model_dict)\n\n    # Remove classification head\n    backbone = nn.Sequential(*list(convnext_tiny_model.children())[:-1])  \n    backbone.out_channels = 768  # ConvNeXt-Tiny feature map channels\n\n    return backbone\n\n# Define Faster R-CNN model\ndef get_faster_rcnn_model():\n    backbone = get_convnext_tiny_backbone()\n\n    # Define anchor generator\n    anchor_generator = AnchorGenerator(\n        sizes=((32, 64, 128, 256, 512),),\n        aspect_ratios=((0.5, 1.0, 2.0),) * 5\n    )\n\n    # Define RoI Pooling\n    roi_pooler = torchvision.ops.MultiScaleRoIAlign(\n        featmap_names=[\"0\"], output_size=7, sampling_ratio=2\n    )\n\n    # Define Faster R-CNN Model\n    model = FasterRCNN(\n        backbone,\n        num_classes=5,  # 4 disease classes + 1 background\n        rpn_anchor_generator=anchor_generator,\n        box_roi_pool=roi_pooler\n    )\n\n    return model.to(device)\n\n# Load trained model\nmodel = get_faster_rcnn_model()\nmodel.eval()\nprint(\"Model loaded successfully!\")\n\n# Preprocess image\ndef preprocess_image(image_path):\n    image = Image.open(image_path).convert(\"RGB\")\n\n    transform = transforms.Compose([\n        transforms.ToTensor(),  \n        transforms.Normalize(mean=[0.480, 0.448, 0.397], std=[0.229, 0.224, 0.225])\n    ])\n\n    return transform(image).to(device).unsqueeze(0), np.array(image)\n\n# Remove background using thresholding\ndef remove_background(image, feature_map):\n    feature_map = feature_map.squeeze().cpu().numpy()\n    feature_map = (feature_map - feature_map.min()) / (feature_map.max() - feature_map.min())  # Normalize\n\n    threshold = 0.3  # Adjust as needed\n    mask = feature_map > threshold  # Create mask\n    image[~mask] = 0  # Set background pixels to black\n\n    return image\n\n# Calculate affected area percentage\ndef calculate_affected_area(boxes, image_shape):\n    total_pixels = image_shape[0] * image_shape[1]\n    affected_pixels = sum((x2 - x1) * (y2 - y1) for x1, y1, x2, y2 in boxes)\n    return (affected_pixels / total_pixels) * 100\n\n# Run detection pipeline\ndef detect_disease(image_path):\n    image_tensor, original_image = preprocess_image(image_path)\n\n    # Get predictions\n    with torch.no_grad():\n        predictions = model(image_tensor)\n\n    boxes = predictions[0]['boxes'].cpu().numpy()\n    scores = predictions[0]['scores'].cpu().numpy()\n    labels = predictions[0]['labels'].cpu().numpy()\n\n    # Remove low-confidence detections\n    confidence_threshold = 0.5\n    valid_indices = scores > confidence_threshold\n    boxes, scores, labels = boxes[valid_indices], scores[valid_indices], labels[valid_indices]\n\n    # Extract feature map from ConvNeXt\n    convnext_model = get_convnext_tiny_backbone()\n    with torch.no_grad():\n        feature_map = convnext_model(image_tensor)\n\n    # Remove background\n    image_no_bg = remove_background(original_image, feature_map)\n\n    # Calculate affected percentage\n    affected_percentage = calculate_affected_area(boxes, original_image.shape)\n    \n    return image_no_bg, boxes, scores, labels, affected_percentage\n\n# Display results\ndef visualize_results(image_path):\n    image_no_bg, boxes, scores, labels, affected_percentage = detect_disease(image_path)\n\n    fig, ax = plt.subplots(1, figsize=(10, 8))\n    ax.imshow(image_no_bg)\n\n    # Draw bounding boxes\n    for box, score, label in zip(boxes, scores, labels):\n        x1, y1, x2, y2 = box\n        width, height = x2 - x1, y2 - y1\n        rect = patches.Rectangle((x1, y1), width, height, linewidth=2, edgecolor='red', facecolor='none')\n        ax.add_patch(rect)\n\n        text = f\"{CLASS_NAMES[label]} ({score:.2f})\"\n        ax.text(x1, y1 - 5, text, fontsize=10, color='white', bbox=dict(facecolor='red', alpha=0.5))\n\n    # Display affected area percentage\n    plt.title(f\"Disease Affected: {affected_percentage:.2f}%\")\n    plt.axis(\"off\")\n    plt.show()\n\n# Run on sample image\nimage_path = \"/kaggle/input/preprocessedimgs/preprocessed/white_spots/White_spots  (100).jpg\"\nvisualize_results(image_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T06:33:21.940336Z","iopub.execute_input":"2025-03-19T06:33:21.940727Z","iopub.status.idle":"2025-03-19T06:33:23.550257Z","shell.execute_reply.started":"2025-03-19T06:33:21.940702Z","shell.execute_reply":"2025-03-19T06:33:23.549032Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-3-7b607bb5ba8c>:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load('/kaggle/input/featuremodel/tensorflow2/default/1/best_convnext_tiny3.pth', map_location=device)\n","output_type":"stream"},{"name":"stdout","text":"Model loaded successfully!\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-7b607bb5ba8c>\u001b[0m in \u001b[0;36m<cell line: 149>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;31m# Run on sample image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0mimage_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/kaggle/input/preprocessedimgs/preprocessed/white_spots/White_spots  (100).jpg\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m \u001b[0mvisualize_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-3-7b607bb5ba8c>\u001b[0m in \u001b[0;36mvisualize_results\u001b[0;34m(image_path)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;31m# Display results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mvisualize_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0mimage_no_bg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maffected_percentage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetect_disease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-3-7b607bb5ba8c>\u001b[0m in \u001b[0;36mdetect_disease\u001b[0;34m(image_path)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0mconvnext_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_convnext_tiny_backbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mfeature_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvnext_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;31m# Remove background\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    547\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             )\n\u001b[0;32m--> 549\u001b[0;31m         return F.conv2d(\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         )\n","\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same"],"ename":"RuntimeError","evalue":"Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same","output_type":"error"}],"execution_count":3},{"cell_type":"code","source":"import torch\nimport torchvision\nimport torch.nn as nn\nimport torchvision.models as models\nfrom torchvision.models.convnext import convnext_tiny\nfrom torchvision.models.detection import FasterRCNN\nfrom torchvision.models.detection.rpn import AnchorGenerator\n\nimport cv2\nimport numpy as np\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\n# Load device (GPU if available)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Disease class names\nCLASS_NAMES = ['Background', 'Black Gills', 'Black Spot', 'White Spot', 'Healthy']\n\n# Load ConvNeXt-Tiny as the feature extractor\ndef get_convnext_tiny_backbone():\n    convnext_tiny_model = convnext_tiny(weights=None)  # No pretrained weights\n    checkpoint = torch.load('/kaggle/input/featuremodel/tensorflow2/default/1/best_convnext_tiny3.pth', map_location=device)\n    \n    # Load model weights\n    model_dict = convnext_tiny_model.state_dict()\n    checkpoint_dict = {k: v for k, v in checkpoint.items() if k in model_dict}\n    model_dict.update(checkpoint_dict)\n    convnext_tiny_model.load_state_dict(model_dict)\n\n    # Remove the classification head\n    backbone = nn.Sequential(*list(convnext_tiny_model.children())[:-1])  # Remove last layer\n    backbone.out_channels = 768  # ConvNeXt-Tiny last feature map has 768 channels\n    \n    return backbone.to(device)\n\n# Define Faster R-CNN model\ndef get_faster_rcnn_model():\n    backbone = get_convnext_tiny_backbone()\n\n    # Define anchor generator\n    anchor_generator = AnchorGenerator(\n        sizes=((32, 64, 128, 256, 512),),\n        aspect_ratios=((0.5, 1.0, 2.0),) * 5\n    )\n\n    # Define RoI Pooling\n    roi_pooler = torchvision.ops.MultiScaleRoIAlign(\n        featmap_names=[\"0\"], output_size=7, sampling_ratio=2\n    )\n\n    # Define Faster R-CNN Model\n    model = FasterRCNN(\n        backbone,\n        num_classes=5,  # 4 disease classes + 1 background\n        rpn_anchor_generator=anchor_generator,\n        box_roi_pool=roi_pooler\n    )\n\n    return model.to(device)\n\n# Initialize the model\nmodel = get_faster_rcnn_model()\nmodel.eval()  # Set to evaluation mode\n\nprint(\"Model loaded successfully!\")\n\n# Image preprocessing\ndef preprocess_image(image_path):\n    image = cv2.imread(image_path)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert to RGB\n\n    transform = transforms.Compose([\n        transforms.ToTensor(),  # Convert to tensor\n        transforms.Normalize(mean=[0.480, 0.448, 0.397], std=[0.229, 0.224, 0.225])  # Normalize\n    ])\n\n    return transform(image)\n\n# Disease detection function\ndef detect_disease(image_path):\n    image_tensor = preprocess_image(image_path).unsqueeze(0).to(device)  # Move image to GPU/CPU\n\n    # Get feature maps from ConvNeXt-Tiny\n    convnext_model = get_convnext_tiny_backbone()\n    convnext_model.eval()\n\n    with torch.no_grad():\n        feature_map = convnext_model(image_tensor)  # Extract features\n\n    # Remove background using thresholding\n    feature_map_np = feature_map.squeeze().cpu().numpy()\n    feature_map_np[feature_map_np < np.mean(feature_map_np)] = 0  # Zero out low-intensity areas\n\n    # Get Faster R-CNN predictions\n    with torch.no_grad():\n        predictions = model(image_tensor)\n\n    # Extract bounding boxes, scores, and labels\n    boxes = predictions[0]['boxes'].cpu().numpy()\n    scores = predictions[0]['scores'].cpu().numpy()\n    labels = predictions[0]['labels'].cpu().numpy()\n\n    # Filter out low-confidence detections\n    threshold = 0.5\n    valid_indices = scores > threshold\n    boxes = boxes[valid_indices]\n    scores = scores[valid_indices]\n    labels = labels[valid_indices]\n\n    # Compute affected area percentage\n    affected_percentage = calculate_affected_area(boxes, image_path)\n\n    return feature_map_np, boxes, scores, labels, affected_percentage\n\n# Function to calculate affected area percentage\ndef calculate_affected_area(boxes, image_path):\n    image = cv2.imread(image_path)\n    image_area = image.shape[0] * image.shape[1]  # Total image pixels\n\n    disease_area = sum((box[2] - box[0]) * (box[3] - box[1]) for box in boxes)  # Sum of bounding box areas\n\n    return (disease_area / image_area) * 100  # Convert to percentage\n\n# Function to visualize results\ndef visualize_results(image_path):\n    image_no_bg, boxes, scores, labels, affected_percentage = detect_disease(image_path)\n\n    # Load the original image\n    image = cv2.imread(image_path)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n    fig, ax = plt.subplots(1, figsize=(10, 8))\n    ax.imshow(image)\n\n    # Draw bounding boxes with labels\n    for i, (box, score, label) in enumerate(zip(boxes, scores, labels)):\n        x1, y1, x2, y2 = box\n        width, height = x2 - x1, y2 - y1\n\n        # Draw bounding box\n        rect = patches.Rectangle((x1, y1), width, height, linewidth=2, edgecolor='r', facecolor='none')\n        ax.add_patch(rect)\n\n        # Display label and confidence\n        label_text = f\"{CLASS_NAMES[label]}: {score:.2f}\"\n        ax.text(x1, y1 - 10, label_text, color='red', fontsize=12, fontweight='bold', bbox=dict(facecolor='white', alpha=0.7))\n\n    # Display affected area percentage\n    ax.set_title(f\"Detected Diseases - Affected Area: {affected_percentage:.2f}%\")\n    plt.axis('off')\n    plt.show()\n\n# Run on a sample image\nimage_path = \"/kaggle/input/preprocessedimgs/preprocessed/white_spots/White_spots(100)_aug2.jpg\"\nvisualize_results(image_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T06:36:55.711710Z","iopub.execute_input":"2025-03-19T06:36:55.712050Z","iopub.status.idle":"2025-03-19T06:36:56.700177Z","shell.execute_reply.started":"2025-03-19T06:36:55.712025Z","shell.execute_reply":"2025-03-19T06:36:56.699068Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-5-810326bbc424>:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load('/kaggle/input/featuremodel/tensorflow2/default/1/best_convnext_tiny3.pth', map_location=device)\n","output_type":"stream"},{"name":"stdout","text":"Model loaded successfully!\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-810326bbc424>\u001b[0m in \u001b[0;36m<cell line: 157>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;31m# Run on a sample image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0mimage_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/kaggle/input/preprocessedimgs/preprocessed/white_spots/White_spots(100)_aug2.jpg\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m \u001b[0mvisualize_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-5-810326bbc424>\u001b[0m in \u001b[0;36mvisualize_results\u001b[0;34m(image_path)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;31m# Function to visualize results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mvisualize_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m     \u001b[0mimage_no_bg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maffected_percentage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetect_disease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;31m# Load the original image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-810326bbc424>\u001b[0m in \u001b[0;36mdetect_disease\u001b[0;34m(image_path)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;31m# Disease detection function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdetect_disease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0mimage_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Move image to GPU/CPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;31m# Get feature maps from ConvNeXt-Tiny\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-810326bbc424>\u001b[0m in \u001b[0;36mpreprocess_image\u001b[0;34m(image_path)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Convert to RGB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     transform = transforms.Compose([\n","\u001b[0;31merror\u001b[0m: OpenCV(4.10.0) /io/opencv/modules/imgproc/src/color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"],"ename":"error","evalue":"OpenCV(4.10.0) /io/opencv/modules/imgproc/src/color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n","output_type":"error"}],"execution_count":5}]}